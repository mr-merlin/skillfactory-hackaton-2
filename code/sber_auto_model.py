import os
import pickle
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    average_precision_score,
    brier_score_loss,
    f1_score,
    recall_score,
    precision_score,
    balanced_accuracy_score,
    cohen_kappa_score,
    matthews_corrcoef,
)
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split


class SberAutoModel:
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ —Å–∞–π—Ç–µ –°–±–µ—Ä–ê–≤—Ç–æ–ø–æ–¥–ø–∏—Å–∫–∞
    """

    def __init__(self) -> None:
        self.model: Optional[RandomForestClassifier] = None
        self.feature_names: Optional[List[str]] = None
        self.target_actions: Optional[List[str]] = None
        self.scaler: Optional[Any] = None

    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        sessions = pd.read_pickle("../data/ga_sessions.pkl")
        hits = pd.read_pickle("../data/ga_hits.pkl")

        print(f"üìä –°–µ—Å—Å–∏–∏: {sessions.shape}")
        print(f"üìä –•–∏—Ç—ã: {hits.shape}")
        print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∏—Ç–æ–≤: {hits.shape[0]:,}")
        print(
            f"üìä –°—Ä–µ–¥–Ω–µ–µ —Ö–∏—Ç–æ–≤ –Ω–∞ —Å–µ—Å—Å–∏—é: {hits.groupby('session_id')['hit_number'].max().mean():.1f}"
        )

        return sessions, hits

    def define_target_actions(self, hits: pd.DataFrame) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        print("üéØ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è...")

        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π
        unique_events = hits["event_action"].value_counts()

        # C–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
        target_keywords = [
            "–∑–∞—è–≤–∫–∞",
            "–∑–≤–æ–Ω–æ–∫",
            "–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ",
            "callback",
            "–ø–æ–∫—É–ø–∫–∞",
            "order",
            "submit",
            "contact",
            "call",
            "chat",
            "auth",
            "success",
            "request",
            "claim",
            "phone",
            "sms",
            "code",
            "confirm",
            "start_chat",
            "user_message",
            "proactive",
            "invitation",
        ]

        potential_targets = []

        for event in unique_events.index:
            event_lower = str(event).lower()
            for keyword in target_keywords:
                if keyword in event_lower:
                    potential_targets.append((event, unique_events[event]))
                    break

        self.target_actions = [event for event, _ in potential_targets]

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.target_actions)} —Ü–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π")
        print(f"üìã –ü—Ä–∏–º–µ—Ä—ã: {self.target_actions[:5]}")

        total_target_events = hits[hits["event_action"].isin(self.target_actions)].shape[0]
        print(f"üìä –í—Å–µ–≥–æ —Ü–µ–ª–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π: {total_target_events:,}")
        print(f"üìä –î–æ–ª—è —Ü–µ–ª–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π: {total_target_events/len(hits)*100:.1f}%")

        return self.target_actions

    def create_features(self, sessions: pd.DataFrame, hits: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üîß –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏...")

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        if self.target_actions is None:
            raise ValueError(
                "–¶–µ–ª–µ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ define_target_actions."
            )

        hits["is_target"] = hits["event_action"].apply(
            lambda x: 1 if any(key in str(x).lower() for key in self.target_actions) else 0
        )

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Å–µ—Å—Å–∏–∏
        session_metrics = (
            hits.groupby("session_id")
            .agg(
                {
                    "is_target": "max",
                    "hit_number": "count",
                    "hit_page_path": "nunique",
                    "hit_time": lambda x: max(x) - min(x) if len(x) > 1 else 0,
                    "event_action": "nunique",
                }
            )
            .rename(
                columns={
                    "hit_number": "total_hits",
                    "hit_page_path": "unique_pages",
                    "hit_time": "session_duration",
                    "event_action": "unique_events",
                }
            )
        )

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        df = sessions.merge(session_metrics, on="session_id", how="left")
        df["is_target"] = df["is_target"].fillna(0).astype(int)
        df["session_duration"] = df["session_duration"].fillna(0)
        df["unique_events"] = df["unique_events"].fillna(0)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df["visit_datetime"] = pd.to_datetime(
            df["visit_date"].astype(str) + " " + df["visit_time"].astype(str)
        )
        df["visit_hour"] = df["visit_datetime"].dt.hour
        df["visit_weekday"] = df["visit_datetime"].dt.weekday
        df["is_weekend"] = df["visit_weekday"].isin([5, 6]).astype(int)

        # –ù–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df["is_morning"] = df["visit_hour"].between(6, 11).astype(int)
        df["is_afternoon"] = df["visit_hour"].between(12, 17).astype(int)
        df["is_evening"] = df["visit_hour"].between(18, 23).astype(int)
        df["is_night"] = ((df["visit_hour"] >= 0) & (df["visit_hour"] <= 5)).astype(int)
        df["is_workday"] = (~df["is_weekend"]).astype(int)

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        df["is_mobile"] = (df["device_category"] == "mobile").astype(int)
        df["is_android"] = (df["device_os"] == "Android").astype(int)
        df["is_ios"] = (df["device_os"] == "iOS").astype(int)
        df["is_desktop"] = (df["device_category"] == "desktop").astype(int)
        df["is_tablet"] = (df["device_category"] == "tablet").astype(int)

        # –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
        df["is_windows"] = (df["device_os"] == "Windows").astype(int)
        df["is_macos"] = (df["device_os"] == "macOS").astype(int)

        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        print("üåç –°–æ–∑–¥–∞–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")

        # –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä–æ–¥–æ–≤ —Å –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
        city_stats = (
            df.groupby("geo_city")
            .agg(
                {
                    "session_id": "count",
                    "is_target": "sum",
                    "session_duration": "mean",
                    "total_hits": "mean",
                }
            )
            .rename(
                columns={
                    "session_id": "city_sessions",
                    "is_target": "city_conversions",
                    "session_duration": "city_avg_duration",
                    "total_hits": "city_avg_hits",
                }
            )
        )

        city_stats["city_conversion_rate"] = (
            city_stats["city_conversions"] / city_stats["city_sessions"] * 100
        ).round(2)

        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≥–æ—Ä–æ–¥–æ–≤
        city_stats["city_tier"] = pd.cut(
            city_stats["city_conversion_rate"],
            bins=[0, 0.8, 1.2, 1.6, 10],
            labels=["low", "medium", "high", "very_high"],
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        df = df.merge(
            city_stats[
                [
                    "city_conversion_rate",
                    "city_tier",
                    "city_sessions",
                    "city_avg_duration",
                    "city_avg_hits",
                ]
            ],
            left_on="geo_city",
            right_index=True,
            how="left",
        )

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df["city_conversion_rate"] = df["city_conversion_rate"].fillna(0)
        df["city_tier"] = df["city_tier"].fillna("low")
        df["city_sessions"] = df["city_sessions"].fillna(0)
        df["city_avg_duration"] = df["city_avg_duration"].fillna(0)
        df["city_avg_hits"] = df["city_avg_hits"].fillna(0)

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df["is_moscow"] = (df["geo_city"] == "Moscow").astype(int)
        df["is_spb"] = (df["geo_city"] == "Saint Petersburg").astype(int)
        df["is_million_plus"] = (df["city_sessions"] >= 1000).astype(int)
        df["is_regional_center"] = (df["city_sessions"] >= 500).astype(int)

        # –ö–æ–¥–∏—Ä—É–µ–º tier –≥–æ—Ä–æ–¥–æ–≤
        df["city_tier_low"] = (df["city_tier"] == "low").astype(int)
        df["city_tier_medium"] = (df["city_tier"] == "medium").astype(int)
        df["city_tier_high"] = (df["city_tier"] == "high").astype(int)
        df["city_tier_very_high"] = (df["city_tier"] == "very_high").astype(int)

        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
        df["is_paid"] = ~df["utm_medium"].isin(["organic", "referral", "(none)"]).astype(int)
        df["is_organic"] = (df["utm_medium"] == "organic").astype(int)
        df["is_referral"] = (df["utm_medium"] == "referral").astype(int)
        df["is_direct"] = (df["utm_medium"] == "(none)").astype(int)

        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        df["avg_time_per_page"] = df["session_duration"] / df["total_hits"]
        df["avg_time_per_page"] = df["avg_time_per_page"].replace([np.inf, -np.inf], 0)

        # –ù–æ–≤—ã–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df["bounce_rate"] = (df["total_hits"] == 1).astype(int)
        df["deep_engagement"] = (df["unique_pages"] >= 5).astype(int)
        df["long_session"] = (df["session_duration"] > 300).astype(int)
        df["very_long_session"] = (df["session_duration"] > 600).astype(int)
        df["high_activity"] = (df["total_hits"] >= 10).astype(int)
        df["very_high_activity"] = (df["total_hits"] >= 15).astype(int)

        # –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        df["events_per_page"] = df["unique_events"] / df["unique_pages"]
        df["events_per_page"] = df["events_per_page"].replace([np.inf, -np.inf], 0)
        df["engagement_score"] = (
            df["total_hits"] * df["unique_pages"] * df["session_duration"]
        ) / 1000

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ—Å–µ—â–µ–Ω–∏–π
        df["is_returning"] = (df["visit_number"] > 1).astype(int)
        df["is_frequent"] = (df["visit_number"] >= 3).astype(int)

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(df)} —Å–µ—Å—Å–∏–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        return df

    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        print("üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏...")

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_cols = [
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            "visit_number",
            "total_hits",
            "unique_pages",
            "session_duration",
            "unique_events",
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            "visit_hour",
            "visit_weekday",
            "is_weekend",
            "is_workday",
            "is_morning",
            "is_afternoon",
            "is_evening",
            "is_night",
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            "is_mobile",
            "is_android",
            "is_ios",
            "is_desktop",
            "is_tablet",
            "is_windows",
            "is_macos",
            # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
            "is_paid",
            "is_organic",
            "is_referral",
            "is_direct",
            # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            "avg_time_per_page",
            "bounce_rate",
            "deep_engagement",
            "long_session",
            "very_long_session",
            "high_activity",
            "very_high_activity",
            "events_per_page",
            "engagement_score",
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ—Å–µ—â–µ–Ω–∏–π
            "is_returning",
            "is_frequent",
            # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            "is_moscow",
            "is_spb",
            "is_million_plus",
            "is_regional_center",
            "city_conversion_rate",
            "city_avg_duration",
            "city_avg_hits",
            "city_tier_low",
            "city_tier_medium",
            "city_tier_high",
            "city_tier_very_high",
        ]

        X = df[feature_cols].fillna(0)
        Y = df["is_target"]

        self.feature_names = feature_cols

        print(f"üìä –ü—Ä–∏–∑–Ω–∞–∫–∏: {X.shape}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {Y.shape}")
        print(f"üìà –ö–æ–Ω–≤–µ—Ä—Å–∏—è: {Y.mean() * 100:.2f}%")
        print(f"üìä –¶–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π: {Y.sum():,}")
        print(
            f"üåç –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: "
            f"{len([f for f in feature_cols if 'city' in f or 'moscow' in f or 'spb' in f])}"
        )
        temporal_keywords = ["hour", "week", "morning", "afternoon", "evening", "night"]
        temporal_features = [f for f in feature_cols if any(kw in f for kw in temporal_keywords)]
        print(f"‚è∞ –í—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(temporal_features)}")

        return X, Y

    def optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series) -> RandomForestClassifier:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        print("üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã...")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        param_grid = {
            "n_estimators": [100, 200],
            "max_depth": [10, 12],
            "min_samples_split": [50],
            "min_samples_leaf": [20],
        }

        # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)

        # Grid Search —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=2,
            scoring="roc_auc",
            n_jobs=-1,
            verbose=1,
        )

        grid_search.fit(X, y)

        print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}")
        print(f"üìà –õ—É—á—à–∏–π ROC-AUC: {grid_search.best_score_:.4f}")

        return grid_search.best_estimator_

    def train_model(self, X: pd.DataFrame, y: pd.Series) -> float:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("ü§ñ –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.model = self.optimize_hyperparameters(X_train, y_train)

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        roc_auc = float(roc_auc_score(y_test, y_pred_proba))
        precision = float(precision_score(y_test, y_pred))
        recall = float(recall_score(y_test, y_pred))
        f1 = float(f1_score(y_test, y_pred))
        accuracy = float((y_pred == y_test).mean())

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        specificity = float(((y_pred == 0) & (y_test == 0)).sum() / (y_test == 0).sum())
        balanced_acc = float(balanced_accuracy_score(y_test, y_pred))
        kappa = float(cohen_kappa_score(y_test, y_pred))
        mcc = float(matthews_corrcoef(y_test, y_pred))

        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        avg_precision = float(average_precision_score(y_test, y_pred_proba))
        brier = float(brier_score_loss(y_test, y_pred_proba))

        print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
        print(f"üìà ROC-AUC: {roc_auc:.4f}")
        print(f"üìä –¶–µ–ª—å 0.65 –ø—Ä–µ–≤—ã—à–µ–Ω–∞ –Ω–∞ {((roc_auc - 0.65) / 0.65 * 100):.1f}%")

        print("\nüìä –î–ï–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê:")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        print(f"   F1-Score: {f1:.3f}")
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Specificity: {specificity:.3f}")
        print(f"   Balanced Accuracy: {balanced_acc:.3f}")
        print(f"   Cohen's Kappa: {kappa:.3f}")
        print(f"   Matthews Correlation: {mcc:.3f}")
        print(f"   Average Precision: {avg_precision:.3f}")
        print(f"   Brier Score: {brier:.3f}")

        print("\nüìã –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test, y_pred))

        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = pd.DataFrame(
            {
                "feature": self.feature_names,
                "importance": self.model.feature_importances_,
            }
        ).sort_values("importance", ascending=False)

        print("\nüèÜ –¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for _, row in feature_importance.head(20).iterrows():
            print(f"{row['feature']}: {row['importance']:.3f}")

        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        cv_roc_scores = cross_val_score(self.model, X, y, cv=5, scoring="roc_auc")
        cv_precision_scores = cross_val_score(self.model, X, y, cv=5, scoring="precision")
        cv_recall_scores = cross_val_score(self.model, X, y, cv=5, scoring="recall")

        print(f"\nüìä –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø:")
        print(f"   ROC-AUC: {cv_roc_scores.mean():.4f} (+/- {cv_roc_scores.std() * 2:.4f})")
        print(
            f"   Precision: {cv_precision_scores.mean():.3f} (+/- {cv_precision_scores.std() * 2:.3f})"
        )
        print(f"   Recall: {cv_recall_scores.mean():.3f} (+/- {cv_recall_scores.std() * 2:.3f})")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        self.metrics = {
            "roc_auc": roc_auc,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "accuracy": accuracy,
            "specificity": specificity,
            "balanced_accuracy": balanced_acc,
            "kappa": kappa,
            "mcc": mcc,
            "avg_precision": avg_precision,
            "brier_score": brier,
            "cv_roc_mean": cv_roc_scores.mean(),
            "cv_roc_std": cv_roc_scores.std(),
            "cv_precision_mean": cv_precision_scores.mean(),
            "cv_precision_std": cv_precision_scores.std(),
            "cv_recall_mean": cv_recall_scores.mean(),
            "cv_recall_std": cv_recall_scores.std(),
        }

        return roc_auc

    def save_model(self, filename: str = "../build/sber_auto_model.pkl") -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é build –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs("../build", exist_ok=True)

        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {filename}...")

        model_data = {
            "model": self.model,
            "feature_names": self.feature_names,
            "target_actions": self.target_actions,
        }

        with open(filename, "wb") as f:
            pickle.dump(model_data, f)

        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    def load_model(self, filename: str = "sber_auto_model.pkl") -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {filename}...")

        with open(filename, "rb") as f:
            model_data = pickle.load(f)

        self.model = model_data["model"]
        self.feature_names = model_data["feature_names"]
        self.target_actions = model_data["target_actions"]

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    def predict(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            data (dict): –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Å–µ—Å—Å–∏–∏

        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–ª–∏ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")

        if self.feature_names is None:
            raise ValueError("–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df_input = pd.DataFrame([data])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        missing_features = set(self.feature_names) - set(df_input.columns)
        if missing_features:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            for feature in missing_features:
                df_input[feature] = 0

        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        X = df_input[self.feature_names].fillna(0)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.model.predict(X)[0]
        probability = self.model.predict_proba(X)[0][1]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        confidence_level = (
            "–≤—ã—Å–æ–∫–∞—è" if probability > 0.7 else "—Å—Ä–µ–¥–Ω—è—è" if probability > 0.3 else "–Ω–∏–∑–∫–∞—è"
        )

        return {
            "prediction": int(prediction),
            "probability": float(probability),
            "will_convert": bool(prediction),
            "conversion_probability": f"{probability * 100:.2f}%",
            "confidence_level": confidence_level,
        }

    def predict_batch(self, data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫

        Args:
            data_list (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —Å–µ—Å—Å–∏–π

        Returns:
            list: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        results = []
        for i, data in enumerate(data_list):
            try:
                result = self.predict(data)
                result["session_id"] = i
                results.append(result)
            except Exception as e:
                results.append(
                    {
                        "session_id": i,
                        "error": str(e),
                        "prediction": 0,
                        "probability": 0.0,
                        "will_convert": False,
                        "conversion_probability": "0.00%",
                    }
                )
        return results


def train_and_save_model() -> SberAutoModel:
    """–û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –°–±–µ—Ä–ê–≤—Ç–æ–ø–æ–¥–ø–∏—Å–∫–∞")
    print("=" * 60)

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–æ–¥–µ–ª–∏
    model = SberAutoModel()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    model_path = "../build/sber_auto_model.pkl"
    if os.path.exists(model_path):
        print("üìÇ –ù–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∑–∞–≥—Ä—É–∂–∞–µ–º...")
        try:
            model.load_model(model_path)
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            return model
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    sessions, hits = model.load_data()

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
    model.define_target_actions(hits)

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    df = model.create_features(sessions, hits)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X, y = model.prepare_features(df)

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    roc_auc = model.train_model(X, y)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.save_model()

    print("üéâ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    print(f"üìä ROC-AUC: {roc_auc:.4f}")
    print(f"‚úÖ –¶–µ–ª–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å 0.65 " f"{'–¥–æ—Å—Ç–∏–≥–Ω—É—Ç' if roc_auc >= 0.65 else '–ù–ï –¥–æ—Å—Ç–∏–≥–Ω—É—Ç'}")

    return model


if __name__ == "__main__":
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = train_and_save_model()

    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
    print("-" * 40)

    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    test_data = {
        "visit_number": 1,
        "total_hits": 5,
        "unique_pages": 3,
        "session_duration": 120,
        "visit_hour": 14,
        "visit_weekday": 2,
        "is_weekend": 0,
        "is_mobile": 1,
        "is_android": 0,
        "is_ios": 1,
        "is_desktop": 0,
        "is_tablet": 0,
        "is_moscow": 1,
        "is_paid": 1,
        "avg_time_per_page": 24.0,
        "bounce_rate": 0,
        "deep_engagement": 0,
        "long_session": 0,
    }

    result = model.predict(test_data)
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
    print(f"   –ö–æ–Ω–≤–µ—Ä—Å–∏—è: {result['conversion_probability']}")
    print(f"   –ë—É–¥–µ—Ç –ª–∏ –∫–æ–Ω–≤–µ—Ä—Å–∏—è: {result['will_convert']}")
    print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {result['probability']:.4f}")
    print(f"   –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {result['confidence_level']}")
